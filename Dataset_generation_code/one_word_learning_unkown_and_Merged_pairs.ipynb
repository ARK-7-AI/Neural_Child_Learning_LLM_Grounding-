{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtSEEiYhYW4x"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DATA_PATH = \"One_word_learning_Known.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "def try_read(path):\n",
        "    for sep in [\",\", \"\\t\", \"|\"]:\n",
        "        try:\n",
        "            return pd.read_csv(path, sep=sep)\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise ValueError(\"Unable to load file with common separators (,, \\\\t, |).\")\n",
        "\n",
        "df = try_read(DATA_PATH)\n",
        "print(f\"Loaded: {len(df):,} rows | Columns: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubxrmh3wZDJI",
        "outputId": "3113e075-f8e7-4d91-fd87-8a7c8bcd0822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 14,705 rows | Columns: ['adult', 'child', 'child_filtered', 'correctness_score']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byLxmKg_iEZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Adult - Child Columns\n",
        "adult_col = None\n",
        "child_col = None\n",
        "\n",
        "for c in df.columns:\n",
        "    cl = c.lower()\n",
        "    if \"adult\" in cl and adult_col is None:\n",
        "        adult_col = c\n",
        "    if \"child\" in cl and child_col is None:\n",
        "        child_col = c\n",
        "\n",
        "df_filtered = df[[adult_col, child_col]].copy()\n",
        "df_filtered.columns = [\"adult\", \"child\"]\n",
        "print(f\"Filtered Columns :{len(df_filtered):,} rows | Columns: {df_filtered.columns.to_list()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV0EBtRRZG4z",
        "outputId": "a6784c19-952b-4e1c-b4fe-49c8335b8c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Columns :14,705 rows | Columns: ['adult', 'child']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Words child has actually produced\n",
        "child_words = set(df[\"child\"].astype(str).str.lower().unique()) - {\"unknown\"}\n",
        "CDI_words=['light', 'dog', 'hi', 'home', 'kitchen', 'milk', 'girl', 'break', 'tooth', 'flower', 'bed', 'soft', 'duck', 'tv', 'block', 'ouch', 'kiss', 'blanket', 'dish', 'meow', 'ball', 'fast', 'jump', 'spoon', 'candy', 'please', 'sock', 'eye', 'bear', 'sun', 'boat', 'fish', 'feed', 'pants', 'kick', 'grandma', 'cookie', 'night', 'pattycake', 'moon', 'box', 'bread', 'chair', 'rock', 'mouth', 'hand', 'apple', 'clock', \"don't\", 'cat', 'head', 'out', 'phone', 'eyes', 'away', 'mommy', 'lamp', 'how', 'leg', 'bird', 'smile', 'broken', 'lion', 'banana', 'toe', 'nose', 'other', 'plant', 'help', 'push', 'bottle', 'outside', 'dark', 'sing', 'cow', 'truck', 'kitty', 'bus', 'radio', 'rain', 'daddy', 'cereal', 'shoe', 'couch', 'toast', 'mouse', 'juice', 'pretty', 'hurt', 'finish', 'star', 'car', 'me', 'today', 'keys', 'book', 'doll', 'bath', 'who', 'baby', 'wait', 'hair', 'stroller', 'table', 'ear', 'hat', 'some', 'foot', 'cup', 'coat', 'big', 'i', 'babysitter']\n",
        "\n",
        "# 2) CDI words (already have this as CDI_words)\n",
        "CDI_words = set(w.lower() for w in CDI_words)\n",
        "\n",
        "# 3) Candidate \"unknown\" tokens = not in child_words and not in CDI\n",
        "# Example: from some larger English vocab or your adult corpus\n",
        "from collections import Counter\n",
        "\n",
        "adult_tokens = []\n",
        "for sent in df[\"adult\"].astype(str):\n",
        "    adult_tokens.extend(t.lower() for t in sent.split())\n",
        "\n",
        "counts = Counter(adult_tokens)\n",
        "# Keep moderately frequent words that child never said and are not in CDI\n",
        "unknown_token_candidates = [\n",
        "    w for w, c in counts.items()\n",
        "    if w.isalpha() and w not in child_words and w not in CDI_words and c >= 3\n",
        "]\n"
      ],
      "metadata": {
        "id": "1igyexcMiGuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "location_templates = [\n",
        "    \"Do you know where {} is?\",\n",
        "    \"Can you tell me where {} is located?\",\n",
        "    \"Do you remember where {} is?\",\n",
        "    \"Where exactly is {}?\",\n",
        "    \"Can you point out where {} is?\",\n",
        "    \"Do you know the location of {}?\",\n",
        "    \"Where can we find {}?\"\n",
        "]\n",
        "\n",
        "concept_templates = [\n",
        "    \"Do you know what {} means?\",\n",
        "    \"Can you explain what {} is?\",\n",
        "    \"Do you remember what {} is about?\",\n",
        "    \"What does {} mean?\",\n",
        "    \"Could you tell me what {} is?\",\n",
        "    \"What is {} used for?\",\n",
        "    \"How would you describe {}?\"\n",
        "]\n",
        "\n",
        "object_templates = [\n",
        "    \"Do you know what {} is?\",\n",
        "    \"What do you think {} is for?\",\n",
        "    \"Have you heard of {} before?\",\n",
        "    \"Do you know what people use {} for?\"\n",
        "]\n",
        "\n",
        "process_templates = [\n",
        "    \"Do you know how {} works?\",\n",
        "    \"Can you tell me how {} happens?\",\n",
        "    \"Do you understand how {} is done?\"\n",
        "]\n",
        "\n",
        "all_templates = location_templates + concept_templates + object_templates + process_templates\n",
        "\n",
        "def generate_template_unknowns(n=1000):\n",
        "    pairs = []\n",
        "    for _ in range(n):\n",
        "        tmpl = random.choice(all_templates)\n",
        "        token = random.choice(unknown_token_candidates)\n",
        "        adult = tmpl.format(token)\n",
        "        pairs.append({\n",
        "            \"adult\": adult,\n",
        "            \"child\": \"unknown\",\n",
        "            \"source\": \"template\"\n",
        "        })\n",
        "    return pd.DataFrame(pairs)\n",
        "\n",
        "template_unknown_df = generate_template_unknowns(n=1000)\n"
      ],
      "metadata": {
        "id": "f29i3T86ZVcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "uFgEYeTVizTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unknown_pairs_corpus(df, CDI_words, child_words, max_unknowns=4000):\n",
        "    unknown_pairs = []\n",
        "    for _, row in df.iterrows():\n",
        "        adult_sentence = str(row[\"adult\"])\n",
        "        adult_l = adult_sentence.lower()\n",
        "        doc = nlp(adult_l)\n",
        "        tokens = [t.text.lower() for t in doc if t.is_alpha]\n",
        "\n",
        "        # Key candidates: tokens not in child_words and not in CDI_words\n",
        "        key_candidates = [\n",
        "            tok for tok in tokens\n",
        "            if tok not in child_words and tok not in CDI_words\n",
        "        ]\n",
        "\n",
        "        # If there is at least one such token, treat utterance as \"unknown\"\n",
        "        if key_candidates:\n",
        "            unknown_pairs.append({\n",
        "                \"adult\": row[\"adult\"],\n",
        "                \"child\": \"unknown\",\n",
        "                \"source\": \"corpus_loose\"\n",
        "            })\n",
        "\n",
        "        if len(unknown_pairs) >= max_unknowns:\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(unknown_pairs)\n",
        "\n",
        "corpus_unknown_df = generate_unknown_pairs_corpus(\n",
        "    df_filtered, CDI_words, child_words, max_unknowns=4000\n",
        ")\n",
        "print(len(corpus_unknown_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWK5etGWimiW",
        "outputId": "c5dd2fe2-8d59-4667-b9af-87d45a5315ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "known_df = df_filtered.iloc[:11000].copy()\n",
        "\n",
        "\n",
        "# corpus_unknown_df already has 'adult' and 'child' (with child=\"unknown\")\n",
        "one_word_learning_df = pd.concat(\n",
        "    [known_df[[\"adult\", \"child\"]], corpus_unknown_df[[\"adult\", \"child\"]],template_unknown_df[[\"adult\",\"child\"]]],\n",
        "    ignore_index=True\n",
        ")\n",
        "one_word_learning_df.to_csv(\"one_word_learning.csv\", index=False)\n",
        "print(len(one_word_learning_df))\n"
      ],
      "metadata": {
        "id": "QaWtCny5bMwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b527b89-f30a-407f-b900-5fc204dd2a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    }
  ]
}